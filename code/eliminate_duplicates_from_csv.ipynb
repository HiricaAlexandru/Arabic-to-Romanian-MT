{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vszP0B_7ut2b",
        "outputId": "de1d8af3-4f06-4e23-f61b-5c82e562e1b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soBzQG3cu-Ft",
        "outputId": "0216c160-df30-462c-fa9c-e47f082b49dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import random\n",
        "import hashlib\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "class FilterDataset:\n",
        "    @staticmethod\n",
        "    def eliminate_sentences_lower_than_size(pandas_dataset, column_to_use, min_length = 5, max_length = 50):\n",
        "\n",
        "        pandas_dataset['number_of_words'] = pandas_dataset[column_to_use].apply(lambda x: len([word for word in word_tokenize(x) if word.isalpha()]))\n",
        "        indexes_to_drop = pandas_dataset[(pandas_dataset[\"number_of_words\"] <= min_length) | (pandas_dataset[\"number_of_words\"] >= max_length)].index\n",
        "        pandas_dataset_droped = pandas_dataset.drop(indexes_to_drop)\n",
        "        return pandas_dataset_droped.drop(columns = \"number_of_words\")\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_word_pairs(sentence, k = 3):\n",
        "        tokenized_sentence = word_tokenize(sentence)\n",
        "        tokenized_sentence = [word.lower() for word in tokenized_sentence if word.isalpha()]\n",
        "        tokenized_parts = set()\n",
        "        for i in range(len(tokenized_sentence) - k + 1):\n",
        "            word_pair = ' '.join(tokenized_sentence[i:i+k])\n",
        "            tokenized_parts.add(word_pair)\n",
        "        return tokenized_parts\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_hash_functions(number_of_hash_functions = 20):\n",
        "        hash_functions = []\n",
        "        large_prime_number = 104729\n",
        "        for elem in range(number_of_hash_functions):\n",
        "            a = random.randint(1, large_prime_number)\n",
        "            b = random.randint(0, large_prime_number)\n",
        "            hash_function = lambda x, a=a, b=b: ((a*hash(x)+b) % large_prime_number)\n",
        "            hash_functions.append(hash_function)\n",
        "\n",
        "        return hash_functions\n",
        "\n",
        "    @staticmethod\n",
        "    def get_min_hashes_of_tokenized_sentence(word_pairs_sentence, hash_functions):\n",
        "        min_values = []\n",
        "        for hash_function in hash_functions:\n",
        "            min_hash_of_current_function = min([hash_function(pair) for pair in word_pairs_sentence])\n",
        "            min_values.append(str(min_hash_of_current_function))\n",
        "        return min_values\n",
        "\n",
        "    @staticmethod\n",
        "    def get_buckets(hash_list, number_of_elems_per_bucket, number_of_buckets):\n",
        "        buckets = []\n",
        "\n",
        "        for i in range(number_of_buckets):\n",
        "            buckets.append(tuple(hash_list[i * number_of_elems_per_bucket: (i+1) * number_of_elems_per_bucket]))\n",
        "\n",
        "        return buckets\n",
        "\n",
        "    @staticmethod\n",
        "    def get_jaccard_similarity(min_hash_list1, min_hash_list2):\n",
        "        length_of_hash = len(min_hash_list1)\n",
        "        count_same = 0\n",
        "        for i in range(length_of_hash):\n",
        "            if min_hash_list1[i] == min_hash_list2[i]:\n",
        "                count_same += 1\n",
        "\n",
        "        return count_same / length_of_hash\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def create_buckets_and_search_forpotential_duplicates(min_hashes, number_of_buckets = 4, threshold = 0.8):\n",
        "        number_of_elems_per_bucket = len(min_hashes[0]) // number_of_buckets\n",
        "\n",
        "        buckets_dictionary = defaultdict(list)\n",
        "\n",
        "        for idx, hash_list in enumerate(min_hashes):\n",
        "            buckets = FilterDataset.get_buckets(hash_list, number_of_elems_per_bucket, number_of_buckets)\n",
        "\n",
        "            for bucket in buckets:\n",
        "                buckets_dictionary[bucket].append(idx)\n",
        "\n",
        "        potential_duplicates = set()\n",
        "\n",
        "        for bucket_key in tqdm(buckets_dictionary.keys()):\n",
        "            if len(buckets_dictionary[bucket_key]):\n",
        "                for i in range(len(buckets_dictionary[bucket_key]) - 1):\n",
        "                    for j in range(i+1, len(buckets_dictionary[bucket_key])):\n",
        "                        min_hashes_document1, min_hashes_document2 = min_hashes[buckets_dictionary[bucket_key][i]], min_hashes[buckets_dictionary[bucket_key][j]]\n",
        "                        similarity = FilterDataset.get_jaccard_similarity(min_hashes_document1, min_hashes_document2)\n",
        "\n",
        "                        if similarity >= threshold:\n",
        "                            potential_duplicates.add((buckets_dictionary[bucket_key][i], buckets_dictionary[bucket_key][j], similarity))\n",
        "\n",
        "        return potential_duplicates\n",
        "\n",
        "    @staticmethod\n",
        "    def search_potential_similarity_from_pandas_dataset(file_path, column_to_analyze):\n",
        "        hashes = FilterDataset.generate_hash_functions(20)\n",
        "\n",
        "        dataset = pd.read_csv(file_path, encoding = \"utf8\")\n",
        "\n",
        "        tokens = [FilterDataset.generate_word_pairs(dataset[column_to_analyze][i]) for i in range(len(dataset[column_to_analyze]))]\n",
        "        min_hashes = [FilterDataset.get_min_hashes_of_tokenized_sentence(tok, hashes) for tok in tokens]\n",
        "        print(FilterDataset.create_buckets_and_search_forpotential_duplicates(min_hashes))\n",
        "\n",
        "    @staticmethod\n",
        "    def search_potential_similarity_from_in_memory_dataset(dataset, column_to_analyze):\n",
        "        hashes = FilterDataset.generate_hash_functions(20)\n",
        "        original_positions_in_dataset = [] #because some indexes might miss, the iteration will retain what element is at each step\n",
        "        tokens = []\n",
        "        print(\"Tokenizing the input...\")\n",
        "        for i in tqdm(range(len(dataset[column_to_analyze]))):\n",
        "            try:\n",
        "                text_row = dataset[column_to_analyze][i]\n",
        "                token = FilterDataset.generate_word_pairs(text_row)\n",
        "                tokens.append(token)\n",
        "                original_positions_in_dataset.append(i)\n",
        "            except Exception as e:\n",
        "                pass\n",
        "        print(\"Creating the hashes...\")\n",
        "        min_hashes = [FilterDataset.get_min_hashes_of_tokenized_sentence(tok, hashes) for tok in tqdm(tokens)]\n",
        "        print(\"Creating the similarities...\")\n",
        "        similarity = FilterDataset.create_buckets_and_search_forpotential_duplicates(min_hashes, threshold=0.8)\n",
        "        similarity_translated = [(original_positions_in_dataset[elem[0]], original_positions_in_dataset[elem[1]], elem[2]) for elem in similarity]\n",
        "        return similarity_translated\n",
        "\n",
        "    @staticmethod\n",
        "    def drop_indexes_in_dataset(dataset, indexes_duplicates):\n",
        "      indexes_to_eliminate = set()\n",
        "      for i in range(len(indexes_duplicates)):\n",
        "          indexes_to_eliminate.add(indexes_duplicates[i][1])\n",
        "      return dataset.drop(index=list(indexes_to_eliminate))\n"
      ],
      "metadata": {
        "id": "Rq8YRQtiu0TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#from FilterDataset import FilterDataset\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "#dataset = pd.read_csv(\"dataset_opus_nllb.csv\")\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/disertatie/dataset_opus_ccMATRIX.csv\")\n",
        "print(len(dataset))\n",
        "dataset = FilterDataset.eliminate_sentences_lower_than_size(dataset,\"ro\")\n",
        "print(len(dataset))\n",
        "results = FilterDataset.search_potential_similarity_from_in_memory_dataset(dataset, \"ro\")\n",
        "df = FilterDataset.drop_indexes_in_dataset(dataset, results)\n",
        "output_file = \"/content/drive/MyDrive/disertatie/cleaned_dataset_opus_ccMATRIX.csv\"\n",
        "df.to_csv(output_file, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQlMEX_Ru4vL",
        "outputId": "b0f35441-cd53-4f89-8ae3-1e8dc634fb6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5087974\n",
            "4247098\n",
            "Tokenizing the input...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4247098/4247098 [09:45<00:00, 7255.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating the hashes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3621733/3621733 [05:32<00:00, 10881.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating the similarities...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12364753/12364753 [00:16<00:00, 763598.88it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9K293W1-C7s",
        "outputId": "1589aaca-885f-42b6-b302-1bb859ef9910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "636455"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v69su5jTvvb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ASVahfoN-Kf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sRyjG4_V-LXT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}