{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCYMBxJ69uGv",
        "outputId": "1ab1fc8b-7bc6-498e-9277-6e9053709d57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"/content/drive/MyDrive/disertatie/FilteredDatasets/Eliminated_dialects/merged_datasets/\"\n",
        "files = [\"arabic-filtered.txt\", \"romanian-filtered.txt\"]"
      ],
      "metadata": {
        "id": "Gk6B6nEk-FOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file_line_by_line(file_path):\n",
        "    \"\"\"Reads a file line by line and prints each line.\n",
        "\n",
        "    Args:\n",
        "        file_path: The path to the file.\n",
        "    \"\"\"\n",
        "\n",
        "    all_lines = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            all_lines.append(line)\n",
        "\n",
        "    return all_lines"
      ],
      "metadata": {
        "id": "jI5UhExz-aF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "romanian_texts = read_file_line_by_line(f\"{input_path}{files[1]}\")\n",
        "arabic_texts = read_file_line_by_line(f\"{input_path}{files[0]}\")"
      ],
      "metadata": {
        "id": "UyHNf1Ju_NJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def pick_random_indexes(data_list, num_indexes=2100):\n",
        "    \"\"\"Picks random indexes from a list.\n",
        "\n",
        "    Args:\n",
        "        data_list: The list to pick indexes from.\n",
        "        num_indexes: The number of indexes to pick. Defaults to 2100.\n",
        "\n",
        "    Returns:\n",
        "        A list of random indexes.\n",
        "    \"\"\"\n",
        "    total_indexes = len(data_list)\n",
        "    if num_indexes > total_indexes:\n",
        "        num_indexes = total_indexes  # Limit to available indexes\n",
        "\n",
        "    random_indexes = random.sample(range(total_indexes), num_indexes)\n",
        "    return random_indexes"
      ],
      "metadata": {
        "id": "gBn-DNQc_VIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexes_test = pick_random_indexes(romanian_texts, num_indexes = 2100)"
      ],
      "metadata": {
        "id": "uQw3IRAo_bs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "romanian_test_set = []\n",
        "arabic_test_set = []\n",
        "for index in indexes_test:\n",
        "  romanian_test_set.append(romanian_texts[index])\n",
        "  arabic_test_set.append(arabic_texts[index])"
      ],
      "metadata": {
        "id": "pbKW6bKc_2vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_path = \"/content/drive/MyDrive/disertatie/test_set/\"\n",
        "files_test = [\"arabic-test.txt\", \"romanian-test.txt\"]"
      ],
      "metadata": {
        "id": "QXGWC58w_34Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_lines_to_file(file_path, lines):\n",
        "    \"\"\"Writes a list of lines to a file, one line per line.\n",
        "    Avoids adding an extra newline character at the end.\n",
        "\n",
        "    Args:\n",
        "        file_path: The path to the file to write to.\n",
        "        lines: A list of strings to write to the file.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'w') as file:\n",
        "        for index, line in enumerate(lines):\n",
        "            if index < len(lines) - 1:  # Check if it's not the last line\n",
        "                file.write(line)  # Add newline for all but the last line\n",
        "            else:\n",
        "                file.write(line)  # Write the last line without a newline"
      ],
      "metadata": {
        "id": "eItARjRyARd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_lines_to_file(f\"{test_path}{files_test[0]}\", arabic_test_set)\n",
        "write_lines_to_file(f\"{test_path}{files_test[1]}\", romanian_test_set)"
      ],
      "metadata": {
        "id": "2mJV0NeBAu-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transform all datasets to eliminate the test set"
      ],
      "metadata": {
        "id": "LeIzdT3dBCc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "romanian_test_set = read_file_line_by_line(f\"{test_path}{files_test[1]}\")\n",
        "arabic_test_set = read_file_line_by_line(f\"{test_path}{files_test[0]}\")"
      ],
      "metadata": {
        "id": "khlIsMfPA6ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mark_locations_of_test_set(data_list, test_set):\n",
        "    \"\"\"Marks locations of test set elements in the data list and removes them.\n",
        "\n",
        "    Args:\n",
        "        data_list: The list to modify.\n",
        "        test_set: The set of elements to remove from data_list.\n",
        "\n",
        "    Returns:\n",
        "        The modified data_list with test set elements removed.\n",
        "    \"\"\"\n",
        "    test_set = set(test_set)  # Convert test_set to a set for efficient lookup\n",
        "    indexes = [i for i, sent in enumerate(data_list) if sent in test_set]\n",
        "    return indexes\n",
        "\n",
        "def eliminate_indexes(data_list, indexes_to_remove):\n",
        "    \"\"\"Eliminates elements from a list based on their indexes.\n",
        "\n",
        "    Args:\n",
        "        data_list: The list to modify.\n",
        "        indexes_to_remove: A list of indexes to remove from data_list.\n",
        "\n",
        "    Returns:\n",
        "        A new list with the specified indexes removed.\n",
        "    \"\"\"\n",
        "    indexes_to_remove = set(indexes_to_remove)  # Convert to a set for faster lookup\n",
        "    return [data_list[i] for i in range(len(data_list)) if i not in indexes_to_remove]"
      ],
      "metadata": {
        "id": "tBjPI0SOCGPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_names = [\"Eliminated_dialects\", \"GEMINI_FILTERED\", \"LEALLA+length+duplicates\", \"WithoutDuplicates+length\"]\n",
        "romanian_file_name = \"romanian-filtered.txt\"\n",
        "arabic_file_name = \"arabic-filtered.txt\"\n",
        "\n",
        "romanian_train_name = \"romanian-filtered_train.txt\"\n",
        "arabic_train_name = \"arabic-filtered_train.txt\"\n",
        "\n",
        "for dataset_name in datasets_names:\n",
        "  path_to_files = f\"/content/drive/MyDrive/disertatie/FilteredDatasets/{dataset_name}/merged_datasets/\"\n",
        "  romanian_texts_set = read_file_line_by_line(f\"{path_to_files}{romanian_file_name}\")\n",
        "  arabic_texts_set = read_file_line_by_line(f\"{path_to_files}{arabic_file_name}\")\n",
        "\n",
        "  indexes_to_remove = mark_locations_of_test_set(romanian_texts_set, romanian_test_set)\n",
        "\n",
        "  romanian_texts_wo_test_set = eliminate_indexes(romanian_texts_set, indexes_to_remove)\n",
        "  arabic_texts_wo_test_set = eliminate_indexes(arabic_texts_set, indexes_to_remove)\n",
        "\n",
        "  print(dataset_name, len(romanian_texts_set) - len(romanian_texts_wo_test_set), len(arabic_texts_set) - len(arabic_texts_wo_test_set), len(romanian_texts_wo_test_set), len(arabic_texts_wo_test_set))\n",
        "\n",
        "  write_lines_to_file(f\"{path_to_files}{romanian_train_name}\", romanian_texts_wo_test_set)\n",
        "  write_lines_to_file(f\"{path_to_files}{arabic_train_name}\", arabic_texts_wo_test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPo27WrRBLh3",
        "outputId": "cc870269-21c0-495c-ebe1-c7475362041d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eliminated_dialects 2156 2156 5438655 5438655\n",
            "GEMINI_FILTERED 2162 2162 7445626 7445626\n",
            "LEALLA+length+duplicates 2178 2178 10843768 10843768\n",
            "WithoutDuplicates+length 2193 2193 13576037 13576037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q3RQ-nZgC1b0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}