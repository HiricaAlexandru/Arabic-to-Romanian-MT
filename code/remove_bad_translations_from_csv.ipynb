{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrhUo8mwf7GO",
        "outputId": "2756f2b3-5511-4972-e3c3-d66db52aba4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klFEgmo7gBP4"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import random\n",
        "import hashlib\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from transformers import BertModel, BertTokenizerFast\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FilterDataset:\n",
        "    @staticmethod\n",
        "    def eliminate_sentences_lower_than_size(pandas_dataset, column_to_use, min_length = 5, max_length = 50):\n",
        "\n",
        "        pandas_dataset['number_of_words'] = pandas_dataset[column_to_use].apply(lambda x: len([word for word in word_tokenize(x) if word.isalpha()]))\n",
        "        indexes_to_drop = pandas_dataset[(pandas_dataset[\"number_of_words\"] <= min_length) | (pandas_dataset[\"number_of_words\"] >= max_length)].index\n",
        "        pandas_dataset_droped = pandas_dataset.drop(indexes_to_drop)\n",
        "        return pandas_dataset_droped.drop(columns = \"number_of_words\")\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_word_pairs(sentence, k = 3):\n",
        "        tokenized_sentence = word_tokenize(sentence)\n",
        "        tokenized_sentence = [word.lower() for word in tokenized_sentence if word.isalpha()]\n",
        "        tokenized_parts = set()\n",
        "        for i in range(len(tokenized_sentence) - k + 1):\n",
        "            word_pair = ' '.join(tokenized_sentence[i:i+k])\n",
        "            tokenized_parts.add(word_pair)\n",
        "        return tokenized_parts\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_hash_functions(number_of_hash_functions = 20):\n",
        "        hash_functions = []\n",
        "        large_prime_number = 104729\n",
        "        for elem in range(number_of_hash_functions):\n",
        "            a = random.randint(1, large_prime_number)\n",
        "            b = random.randint(0, large_prime_number)\n",
        "            hash_function = lambda x, a=a, b=b: ((a*hash(x)+b) % large_prime_number)\n",
        "            hash_functions.append(hash_function)\n",
        "\n",
        "        return hash_functions\n",
        "\n",
        "    @staticmethod\n",
        "    def get_min_hashes_of_tokenized_sentence(word_pairs_sentence, hash_functions):\n",
        "        min_values = []\n",
        "        for hash_function in hash_functions:\n",
        "            min_hash_of_current_function = min([hash_function(pair) for pair in word_pairs_sentence])\n",
        "            min_values.append(str(min_hash_of_current_function))\n",
        "        return min_values\n",
        "\n",
        "    @staticmethod\n",
        "    def get_buckets(hash_list, number_of_elems_per_bucket, number_of_buckets):\n",
        "        buckets = []\n",
        "\n",
        "        for i in range(number_of_buckets):\n",
        "            buckets.append(tuple(hash_list[i * number_of_elems_per_bucket: (i+1) * number_of_elems_per_bucket]))\n",
        "\n",
        "        return buckets\n",
        "\n",
        "    @staticmethod\n",
        "    def get_jaccard_similarity(min_hash_list1, min_hash_list2):\n",
        "        length_of_hash = len(min_hash_list1)\n",
        "        count_same = 0\n",
        "        for i in range(length_of_hash):\n",
        "            if min_hash_list1[i] == min_hash_list2[i]:\n",
        "                count_same += 1\n",
        "\n",
        "        return count_same / length_of_hash\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def create_buckets_and_search_forpotential_duplicates(min_hashes, number_of_buckets = 4, threshold = 0.8):\n",
        "        number_of_elems_per_bucket = len(min_hashes[0]) // number_of_buckets\n",
        "\n",
        "        buckets_dictionary = defaultdict(list)\n",
        "\n",
        "        for idx, hash_list in enumerate(min_hashes):\n",
        "            buckets = FilterDataset.get_buckets(hash_list, number_of_elems_per_bucket, number_of_buckets)\n",
        "\n",
        "            for bucket in buckets:\n",
        "                buckets_dictionary[bucket].append(idx)\n",
        "\n",
        "        potential_duplicates = set()\n",
        "\n",
        "        for bucket_key in tqdm(buckets_dictionary.keys()):\n",
        "            if len(buckets_dictionary[bucket_key]):\n",
        "                for i in range(len(buckets_dictionary[bucket_key]) - 1):\n",
        "                    for j in range(i+1, len(buckets_dictionary[bucket_key])):\n",
        "                        min_hashes_document1, min_hashes_document2 = min_hashes[buckets_dictionary[bucket_key][i]], min_hashes[buckets_dictionary[bucket_key][j]]\n",
        "                        similarity = FilterDataset.get_jaccard_similarity(min_hashes_document1, min_hashes_document2)\n",
        "\n",
        "                        if similarity >= threshold:\n",
        "                            potential_duplicates.add((buckets_dictionary[bucket_key][i], buckets_dictionary[bucket_key][j], similarity))\n",
        "\n",
        "        return potential_duplicates\n",
        "\n",
        "    @staticmethod\n",
        "    def search_potential_similarity_from_pandas_dataset(file_path, column_to_analyze):\n",
        "        hashes = FilterDataset.generate_hash_functions(20)\n",
        "\n",
        "        dataset = pd.read_csv(file_path, encoding = \"utf8\")\n",
        "\n",
        "        tokens = [FilterDataset.generate_word_pairs(dataset[column_to_analyze][i]) for i in range(len(dataset[column_to_analyze]))]\n",
        "        min_hashes = [FilterDataset.get_min_hashes_of_tokenized_sentence(tok, hashes) for tok in tokens]\n",
        "        print(FilterDataset.create_buckets_and_search_forpotential_duplicates(min_hashes))\n",
        "\n",
        "    @staticmethod\n",
        "    def search_potential_similarity_from_in_memory_dataset(dataset, column_to_analyze):\n",
        "        hashes = FilterDataset.generate_hash_functions(20)\n",
        "        original_positions_in_dataset = [] #because some indexes might miss, the iteration will retain what element is at each step\n",
        "        tokens = []\n",
        "        print(\"Tokenizing the input...\")\n",
        "        for i in tqdm(range(len(dataset[column_to_analyze]))):\n",
        "            try:\n",
        "                text_row = dataset[column_to_analyze][i]\n",
        "                token = FilterDataset.generate_word_pairs(text_row)\n",
        "                tokens.append(token)\n",
        "                original_positions_in_dataset.append(i)\n",
        "            except Exception as e:\n",
        "                pass\n",
        "        print(\"Creating the hashes...\")\n",
        "        min_hashes = [FilterDataset.get_min_hashes_of_tokenized_sentence(tok, hashes) for tok in tqdm(tokens)]\n",
        "        print(\"Creating the similarities...\")\n",
        "        similarity = FilterDataset.create_buckets_and_search_forpotential_duplicates(min_hashes, threshold=0.8)\n",
        "        similarity_translated = [(original_positions_in_dataset[elem[0]], original_positions_in_dataset[elem[1]], elem[2]) for elem in similarity]\n",
        "        return similarity_translated\n",
        "\n",
        "    @staticmethod\n",
        "    def drop_indexes_in_dataset(dataset, indexes_duplicates):\n",
        "      indexes_to_eliminate = set()\n",
        "      for i in range(len(indexes_duplicates)):\n",
        "          indexes_to_eliminate.add(indexes_duplicates[i][1])\n",
        "      return dataset.drop(index=list(indexes_to_eliminate))\n",
        "\n",
        "    @staticmethod\n",
        "    def similarity(embeddings_1, embeddings_2):\n",
        "      normalized_embeddings_1 = F.normalize(embeddings_1, p=2)\n",
        "      normalized_embeddings_2 = F.normalize(embeddings_2, p=2)\n",
        "      return torch.matmul(normalized_embeddings_1, normalized_embeddings_2.transpose(0, 1))\n",
        "\n",
        "    @staticmethod\n",
        "    def mark_locations_with_bad_translations(dataset, threshold, batch = 1024):\n",
        "      tokenizer = BertTokenizerFast.from_pretrained(\"setu4993/LEALLA-base\")\n",
        "      model = BertModel.from_pretrained(\"setu4993/LEALLA-base\")\n",
        "      model = model.eval()\n",
        "      if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "      else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "      model = model.to(device)\n",
        "\n",
        "      romanian_sentences = list(dataset[\"ro\"])\n",
        "      arabic_sentences = list(dataset[\"ar\"])\n",
        "\n",
        "      indexes_to_eliminate = []\n",
        "\n",
        "      print(\"Processing batch..\")\n",
        "      #go batch by batch\n",
        "      for i in tqdm(range(0, len(romanian_sentences), batch)):\n",
        "        romanian_batch = romanian_sentences[i:i+batch]\n",
        "        arabic_batch = arabic_sentences[i:i+batch]\n",
        "\n",
        "        romanian_inputs = tokenizer(romanian_batch, return_tensors=\"pt\", truncation = True, padding=True).to(device)\n",
        "        arabic_inputs = tokenizer(arabic_batch, return_tensors=\"pt\", truncation = True, padding=True).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            romanian_outputs = model(**romanian_inputs)\n",
        "            arabic_outputs = model(**arabic_inputs)\n",
        "\n",
        "        measured_similarity_pairs = FilterDataset.similarity(romanian_outputs.pooler_output, arabic_outputs.pooler_output)\n",
        "        diagonal_similarities = torch.diagonal(measured_similarity_pairs)\n",
        "        indexes = (diagonal_similarities < threshold).nonzero(as_tuple=True)[0].tolist()\n",
        "        for index in indexes:\n",
        "            indexes_to_eliminate.append(i + index)\n",
        "\n",
        "        del romanian_inputs, arabic_inputs, romanian_outputs, arabic_outputs, measured_similarity_pairs\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "      return indexes_to_eliminate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXH0q2DZgx8u",
        "outputId": "7b066589-82bd-48cf-db32-f59bae66cad9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 2555/5271 [1:29:12<1:29:49,  1.98s/it]"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/disertatie/cleaned_dataset_opus_subtitles.csv\")\n",
        "indexes = FilterDataset.mark_locations_with_bad_translations(data, 0.4, 2056)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joM8-2CBs9ko"
      },
      "outputs": [],
      "source": [
        "new_data = data.drop(index=indexes)\n",
        "new_data.to_csv(\"/content/drive/MyDrive/disertatie/cleaned_dataset_opus_subtitles_LEALLA_FILTERED.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "GAv7_9Yf82qt",
        "outputId": "e586f155-3792-4fd8-fc96-e0f1bf5071ac"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'indexes' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-4d1fd2de9c67>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindexes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'indexes' is not defined"
          ]
        }
      ],
      "source": [
        "indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI3FBv3o5ase"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}